{
    "code": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport os\n\n# Define output folder\noutput_folder = 'uploads/3af96f2b-3bec-44df-b321-ff628a74bb76'\nos.makedirs(output_folder, exist_ok=True)\n\n# URL to scrape\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch the HTML content\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Find the main table containing the film data\ntables = soup.find_all('table', class_='wikitable')\ndata_table = None\n\n# Iterate through tables to find the one with 'Worldwide gross' header\nfor table in tables:\n    headers = [th.get_text(strip=True) for th in table.find_all('th')]\n    if 'Worldwide gross' in headers and 'Film' in headers and 'Year' in headers:\n        data_table = table\n        break\n\nif data_table:\n    # Extract headers\n    headers = [th.get_text(strip=True) for th in data_table.find_all('th')]\n\n    # Extract rows\n    rows = []\n    for tr in data_table.find_all('tr')[1:]: # Skip header row\n        cells = tr.find_all(['td', 'th']) # Get both td and th (for potential film name in th)\n        row_data = [cell.get_text(strip=True) for cell in cells]\n        # Ensure row has correct number of columns matching headers\n        if len(row_data) == len(headers):\n            rows.append(row_data)\n\n    # Create DataFrame\n    df = pd.DataFrame(rows, columns=headers)\n\n    # Clean 'Worldwide gross' column\n    if 'Worldwide gross' in df.columns:\n        df['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False)\n        df['Worldwide gross'] = df['Worldwide gross'].str.replace(',', '', regex=False)\n        df['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: x.split('(')[0].split('[')[0].strip())\n        df['Worldwide gross'] = pd.to_numeric(df['Worldwide gross'], errors='coerce')\n\n    # Clean 'Year' column\n    if 'Year' in df.columns:\n        df['Year'] = pd.to_numeric(df['Year'].astype(str).str.split('[').str[0].strip(), errors='coerce')\n\n    # Clean 'Rank' column\n    if 'Rank' in df.columns:\n        df['Rank'] = pd.to_numeric(df['Rank'].astype(str).str.split('[').str[0].strip(), errors='coerce')\n\n    # Save to CSV\n    csv_path = os.path.join(output_folder, 'data.csv')\n    df.to_csv(csv_path, index=False)\n\n    # Generate metadata\n    metadata_path = os.path.join(output_folder, 'metadata.txt')\n    with open(metadata_path, 'w') as f:\n        f.write(\"Scraped Data Metadata:\\n\\n\")\n        f.write(\"--- DataFrame Info ---\\n\")\n        df.info(buf=f)\n        f.write(\"\\n\\n--- DataFrame Columns ---\\n\")\n        f.write(str(df.columns.tolist()))\n        f.write(\"\\n\\n--- DataFrame Head ---\\n\")\n        f.write(df.head().to_string())\n        f.write(f\"\\n\\n--- data.csv File Path ---\\n\")\n        f.write(f\"Path: {csv_path}\\n\")\n        f.write(f\"Description: Contains a list of highest-grossing films scraped from Wikipedia, including Rank, Film title, Worldwide gross, and Release Year.\\n\")\n        f.write(\"\\n\\n--- Expected JSON Answer Format for Questions ---\\n\")\n        f.write(\"[\"Answer to Q1\", \"Answer to Q2\", ...]\\n\")\n\nelse:\n    # Handle case where table is not found\n    metadata_path = os.path.join(output_folder, 'metadata.txt')\n    with open(metadata_path, 'w') as f:\n        f.write(\"Error: Could not find the main film table on the Wikipedia page.\\n\")\n    df = pd.DataFrame()\n    csv_path = os.path.join(output_folder, 'data.csv')\n    df.to_csv(csv_path, index=False)\n",
    "libraries": [
        "requests",
        "beautifulsoup4",
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport os\n\n# Define the directory for uploads\nupload_dir = 'uploads/3af96f2b-3bec-44df-b321-ff628a74bb76'\nos.makedirs(upload_dir, exist_ok=True)\n\n# Define the URL to scrape\nurl = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\n# Fetch the webpage content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an exception for bad status codes\n\n# Parse the HTML content\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find the table containing the highest-grossing films\n# The table has a class 'wikitable sortable'\ntable = soup.find('table', {'class': 'wikitable sortable'})\n\nif table:\n    headers = []\n    for th in table.find_all('th'):\n        text = th.get_text(strip=True)\n        # Remove references like [b], [c] from headers\n        text = text.split('[')[0].strip()\n        headers.append(text)\n    \n    # Initialize a list to hold all rows\n    data_rows = []\n    \n    # Iterate over each row in the table body\n    for tr in table.find_all('tr')[1:]: # Skip the header row\n        cells = tr.find_all(['td', 'th']) # Get both td and th for data cells\n        row_data = [cell.get_text(strip=True) for cell in cells]\n        # Clean data for potential missing columns (e.g., if a row has fewer cells than headers)\n        row_data_cleaned = [row_data[i] if i < len(row_data) else None for i in range(len(headers))]\n        data_rows.append(row_data_cleaned)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data_rows, columns=headers)\n    \n    # --- Data Cleaning --- \n    # Focus on 'Worldwide gross', 'Year', 'Rank', 'Peak' if present\n    # Standardize column names if needed\n    column_mapping = {\n        'Worldwide gross': 'Worldwide_Gross',\n        'Year': 'Year',\n        'Rank': 'Rank',\n        'Peak': 'Peak'\n    }\n\n    df = df.rename(columns={col: column_mapping.get(col, col) for col in df.columns})\n    \n    # Clean 'Worldwide_Gross' column\n    if 'Worldwide_Gross' in df.columns:\n        df['Worldwide_Gross'] = df['Worldwide_Gross'].astype(str)\n        df['Worldwide_Gross'] = df['Worldwide_Gross'].str.replace('$', '', regex=False)\n        df['Worldwide_Gross'] = df['Worldwide_Gross'].str.replace(',', '', regex=False)\n        df['Worldwide_Gross'] = df['Worldwide_Gross'].apply(lambda x: x.split('[')[0].strip() if x else None)\n        df['Worldwide_Gross'] = pd.to_numeric(df['Worldwide_Gross'], errors='coerce')\n    \n    # Clean 'Year' column\n    if 'Year' in df.columns:\n        df['Year'] = df['Year'].astype(str)\n        df['Year'] = df['Year'].apply(lambda x: x.split('[')[0].strip() if x else None)\n        df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n        \n    # Clean 'Rank' column\n    if 'Rank' in df.columns:\n        df['Rank'] = df['Rank'].astype(str)\n        df['Rank'] = df['Rank'].apply(lambda x: x.split('[')[0].strip() if x else None)\n        df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n    # Clean 'Peak' column\n    if 'Peak' in df.columns:\n        df['Peak'] = df['Peak'].astype(str)\n        df['Peak'] = df['Peak'].apply(lambda x: x.split('[')[0].strip() if x else None)\n        df['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n    # Save the DataFrame to a CSV file\n    csv_file_path = os.path.join(upload_dir, 'data.csv')\n    df.to_csv(csv_file_path, index=False)\n\n    # Create metadata file\n    metadata_file_path = os.path.join(upload_dir, 'metadata.txt')\n    with open(metadata_file_path, 'w') as f:\n        f.write('Scraped Data Metadata:\\n\\n')\n        f.write(f'File Path: {csv_file_path}\\n')\n        f.write('Description: This CSV file contains a list of highest-grossing films scraped from Wikipedia.\\n\\n')\n        f.write('DataFrame Info:\\n')\n        df.info(buf=f)\n        f.write('\\n\\n')\n        f.write('DataFrame Columns:\\n')\n        f.write(str(df.columns.tolist()) + '\\n\\n')\n        f.write('DataFrame Head (first 5 rows):\\n')\n        f.write(df.head().to_string() + '\\n\\n')\n        f.write('Expected JSON Answer Format for analysis:\\n')\n        f.write('[\"Answer to Q1\", \"Answer to Q2\", \"Answer to Q3\", \"data:image/png;base64,...\"]\\n')\nelse:\n    print(\"Could not find the table on the page.\")\n    # Still create metadata file to indicate failure\n    metadata_file_path = os.path.join(upload_dir, 'metadata.txt')\n    with open(metadata_file_path, 'w') as f:\n        f.write('Scraping Failed:\\n\\n')\n        f.write(f'Could not find the table on the page at {url}.\\n\\n')\n        f.write('Expected JSON Answer Format for analysis:\\n')\n        f.write('[\"Answer to Q1\", \"Answer to Q2\", \"Answer to Q3\", \"data:image/png;base664,...\"]\\n')\n",
    "libraries": [
        "requests",
        "beautifulsoup4",
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0KG...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}